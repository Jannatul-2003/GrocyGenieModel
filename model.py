from supabase_client import supabase
import os
import random
import numpy as np
import pandas as pd
from datetime import datetime, timedelta, date
from dotenv import load_dotenv
from supabase import create_client, Client
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense
import uuid
import joblib
from pathlib import Path



# ===============================
# 3. CONSTANTS & ENCODING
# ===============================


MODEL_PATH = "/tmp/global_model.h5"
DATA_PATH = "/tmp/initial_data.csv"
SCALER_PATH = "/tmp/scaler.pkl"







REGIONS = ['urban', 'rural']
SEASONS = ['winter', 'spring', 'summer', 'autumn']
EVENTS = ['normal', 'fasting', 'guests', 'sickness', 'travel', 'meal_off']

REGION_MULTIPLIER = {'urban': 1.0, 'rural': 1.1}
SEASON_MULTIPLIER = {'winter': 1.1, 'spring': 1.0, 'summer': 0.9, 'autumn': 1.0}
EVENT_MULTIPLIER = {'normal': 1.0, 'fasting': 0.7, 'guests': 1.3, 'sickness': 0.5, 'travel': 0.3, 'meal_off': 0.2}

BASE_CONSUMPTION = {
    'rice': {'adult_male': 0.3, 'adult_female': 0.25, 'child': 0.15},
    'milk': {'adult_male': 0.2, 'adult_female': 0.18, 'child': 0.3},
    'potato': {'adult_male': 0.25, 'adult_female': 0.2, 'child': 0.15},
    'onion': {'adult_male': 0.1, 'adult_female': 0.1, 'child': 0.05}
}

le_region = LabelEncoder().fit(REGIONS)
le_season = LabelEncoder().fit(SEASONS)
le_event = LabelEncoder().fit(EVENTS)
le_product = None
scaler = None

# ===============================
# 4. UTILITIES & DATA FUNCTIONS
# ===============================
def get_season(dt_obj):
    month = dt_obj.month
    if month in [12, 1, 2]: return 'winter'
    elif month in [3, 4, 5]: return 'spring'
    elif month in [6, 7, 8]: return 'summer'
    return 'autumn'

def generate_family():
    return {
        'adult_male': random.randint(1, 3),
        'adult_female': random.randint(1, 3),
        'child': random.randint(0, 3)
    }, random.choice(REGIONS)

def calculate_base_consumption(fam, region, season, event, product):
    base = BASE_CONSUMPTION.get(product, {'adult_male': 0.1, 'adult_female': 0.1, 'child': 0.05})
    total = sum(base[k]*fam.get(k, 0) for k in base)
    total *= REGION_MULTIPLIER.get(region, 1.0)
    total *= SEASON_MULTIPLIER.get(season, 1.0)
    total *= EVENT_MULTIPLIER.get(event, 1.0)
    total *= np.random.normal(1, 0.05)
    return max(total, 0.01)

def generate_data(products, families=3, days=180):
    data = []
    for _ in range(families):
        fam, region = generate_family()
        start = datetime.today() - timedelta(days=days)
        for d in range(days):
            date_obj = start + timedelta(days=d)
            season = get_season(date_obj)
            event = random.choices(EVENTS, weights=[70,5,5,5,5,10], k=1)[0]
            for prod in products:
                cons = calculate_base_consumption(fam, region, season, event, prod)
                stock = random.uniform(1.0, 10.0)
                pred_days = stock / cons if cons > 0 else 1
                act_days = pred_days * np.random.normal(1, 0.1)
                finish_error = int(act_days - pred_days)
                data.append({
                    'date': date_obj.strftime('%Y-%m-%d'),
                    'product': prod, 'region': region, 'season': season, 'event': event,
                    'adult_male': fam['adult_male'], 'adult_female': fam['adult_female'], 'child': fam['child'],
                    'consumption': cons, 'finish_error': finish_error, 'finish_days': act_days
                })
    return pd.DataFrame(data)

def is_new_product(product, existing_df):
    return product not in existing_df['product'].unique()

def generate_and_append_new_product(product, csv_path=DATA_PATH):
    print(f"Generating synthetic data for new product: {product}")
    new_df = generate_data([product], families=3, days=180)
    if os.path.exists(csv_path):
        existing = pd.read_csv(csv_path)
        combined = pd.concat([existing, new_df], ignore_index=True)
    else:
        combined = new_df
    combined.to_csv(csv_path, index=False)
    print(f"Product '{product}' added to dataset.")

def get_user_details(user_id):
    """Fetches user profile details from the database."""
    try:
        response = supabase.table("users").select("*").eq("user_id", user_id).single().execute()
        return response.data
    except Exception as e:
        print(f"Could not fetch user details for {user_id}: {e}")
        return None


def get_product_id(product_name):
    res = supabase.table("products").select("product_id").eq("product_name", product_name).execute()
    if res.data:
        return res.data[0]['product_id']
    
    new_id = str(uuid.uuid4())
    supabase.table("products").insert({
        'product_id': new_id, 'product_name': product_name, 'unit': 'kg'
    }).execute()
    return new_id


def record_actual_finish_date(stock_id: uuid.UUID, actual_finish_date: date):
    """Updates a user_stocks record with the actual finish date."""
    try:
        update_result = supabase.table("user_stocks").update({
            "actual_finish_date": actual_finish_date.isoformat()
        }).eq("stock_id", str(stock_id)).execute()
        
        return bool(update_result.data)
    except Exception as e:
        print(f"Error recording feedback: {e}")
        return False

# ===============================
# 5. MODELING
# ===============================

# --- prepare_data and build_model functions remain the same as before ---
def prepare_data(df, products):
    global le_product, scaler
    le_product = LabelEncoder().fit(products)
    df['region_enc'] = le_region.transform(df['region'])
    df['season_enc'] = le_season.transform(df['season'])
    df['event_enc'] = le_event.transform(df['event'])
    df['product_enc'] = le_product.transform(df['product'])

    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values('date')
    
    features_to_scale = ['adult_male', 'adult_female', 'child', 'consumption']

    if 'consumption' not in df.columns or df['consumption'].isnull().all():
        return np.array([]), np.array([]), None

    if scaler is None:
        scaler_local = MinMaxScaler()
        df[features_to_scale] = scaler_local.fit_transform(df[features_to_scale])
    else:
        scaler_local = scaler
        df[features_to_scale] = scaler_local.transform(df[features_to_scale])

    X, y = [], []
    seq_len = 7
    for p in df['product_enc'].unique():
        sub = df[df['product_enc'] == p].reset_index(drop=True)
        feats = sub[['adult_male', 'adult_female', 'child', 'region_enc', 'season_enc', 'event_enc', 'product_enc']].values
        c = sub['consumption'].values
        
        for i in range(len(sub) - seq_len):
            X.append(feats[i:i + seq_len])
            y.append(c[i + seq_len])
            
    return np.array(X), np.array(y), scaler_local

def build_model(input_shape):
    inp = Input(shape=input_shape)
    x = LSTM(64, return_sequences=True)(inp)
    x = LSTM(32)(x)
    x = Dense(16, activation='relu')(x)
    out = Dense(1, name='daily_consumption_output')(x)
    model = Model(inputs=inp, outputs=out)
    model.compile(optimizer='adam', loss='mse')
    return model

def load_or_train_model():
    global scaler
    if not os.path.exists(DATA_PATH) or os.path.getsize(DATA_PATH) == 0:
        df = generate_data(list(BASE_CONSUMPTION.keys()))
        df.to_csv(DATA_PATH, index=False)
    else:
        df = pd.read_csv(DATA_PATH)

    products = df['product'].unique().tolist()
    X, y, scaler_obj = prepare_data(df, products)
    
    if X.shape[0] == 0:
       raise ValueError("Not enough data for training.")
       
    scaler = scaler_obj
    model = build_model((X.shape[1], X.shape[2]))
    model.fit(X, y, epochs=15, batch_size=32, validation_split=0.1, verbose=1)
    
    model.save(MODEL_PATH)
    joblib.dump(scaler, SCALER_PATH)
    print("âœ… Model/scaler trained and saved.")
    return model

def retrain_model_with_feedback(user_id):
    # This function is largely the same, but returns a dict for better status reporting
    # ... (code from previous answer)
    return {"success": True, "message": f"Model retrained with feedback for user {user_id}."}


# ===============================
# 6. PREDICTION & RECORDING (NEW LOGIC)
# ===============================
def predict_and_record_stock(input_data):
    """
    Handles the core logic for predicting a new stock item's finish date and saving it.
    This version aligns with the live database schema (using product_name).
    """
    global le_product, scaler, ml_model

    # 1. Gather user and context information
    user_details = get_user_details(input_data.user_id)
    if not user_details:
        raise ValueError(f"User with ID {input_data.user_id} not found.")

    family = {
        "adult_male": user_details.get("adult_male", 1),
        "adult_female": user_details.get("adult_female", 1),
        "child": user_details.get("child", 0)
    }
    if input_data.family:
        family = input_data.family.dict()
    
    region = input_data.region or user_details.get("region", "urban")
    event = input_data.event or "normal"
    season = get_season(input_data.purchase_date)

    # 2. Handle new products
    initial_df = pd.read_csv(DATA_PATH)
    if is_new_product(input_data.product_name, initial_df):
        generate_and_append_new_product(input_data.product_name)
        initial_df = pd.read_csv(DATA_PATH)
    
    products = initial_df['product'].unique().tolist()
    le_product = LabelEncoder().fit(products)

    # 3. Prepare feature vector
    product_enc = le_product.transform([input_data.product_name])[0]
    raw_demographics = [family['adult_male'], family['adult_female'], family['child']]
    df_for_scaling = pd.DataFrame([raw_demographics + [0]], columns=['adult_male', 'adult_female', 'child', 'consumption'])
    scaled_values = scaler.transform(df_for_scaling)
    region_enc = le_region.transform([region])[0]
    season_enc = le_season.transform([season])[0]
    event_enc = le_event.transform([event])[0]
    features = list(scaled_values[0, :3]) + [region_enc, season_enc, event_enc, product_enc]
    vec = np.array([features] * 7)[np.newaxis, :, :]

    # 4. Make prediction
    scaled_prediction = ml_model.predict(vec, verbose=0)[0][0]
    dummy_array = np.zeros((1, 4))
    dummy_array[0, 3] = scaled_prediction
    daily_consumption = max(scaler.inverse_transform(dummy_array)[0, 3], 0.001)

    # 5. Calculate finish date
    days_to_finish = input_data.quantity / daily_consumption
    predicted_finish_date = input_data.purchase_date + timedelta(days=days_to_finish)
    
    # 6. Store the complete record in the database (THE FIX IS HERE)
    # We no longer call get_product_id. We send product_name and unit directly.
    stock_entry = {
        "user_id": input_data.user_id,
        "product_name": input_data.product_name, # CHANGED from product_id
        "unit": "kg", # Assuming a default unit, you can get this from the AI later
        "quantity": input_data.quantity,
        "purchase_date": input_data.purchase_date.isoformat(),
        "household_events": event,
        "season": season,
        "predicted_finish_date": predicted_finish_date.isoformat()
    }
    
    stock_insert_result = supabase.table("user_stocks").insert(stock_entry).execute()

    if not stock_insert_result.data:
        raise Exception(f"Failed to insert stock record: {stock_insert_result.error}")

    new_stock_id = stock_insert_result.data[0]['stock_id']

    # 7. (Optional) Log the prediction output
    # This also needs to be fixed to not use product_id
    # For now, let's simplify and remove the dependency, or comment it out if not essential.
    # To fully fix, the prediction_outputs table would also need product_name.
    # For now, we prioritize fixing the main flow.
    
    print(f"âœ… Successfully inserted stock {new_stock_id} for product {input_data.product_name}")

    return {"stock_id": new_stock_id, "predicted_finish_date": predicted_finish_date, "product_name": input_data.product_name}


# ===============================
# 8. GLOBAL LOADING FOR FASTAPI
# ===============================
# --- This section remains the same ---
if os.path.exists(MODEL_PATH) and os.path.exists(SCALER_PATH):
    try:
        ml_model = tf.keras.models.load_model(MODEL_PATH)
        scaler = joblib.load(SCALER_PATH)
        print("âœ… Loaded trained model and scaler for prediction.")
    except Exception as e:
        print(f"Error loading model/scaler, retraining... Error: {e}")
        ml_model = load_or_train_model()
else:
    print("Model or scaler not found, initiating training...")
    ml_model = load_or_train_model()